# Dense and ReLU layer

The Dense layer is the computation of the inner product between a set of trainable weights (weight matrix) and an input vector.  The visualization of the dense layer could be seen in the image below. 

![](p37X8J3HSDW-1_Cdx1g15w_e536ba17aee74f9c8828414017aec8c8_Screen-Sh.png)

The orange box shows the dense layer. An activation layer is the set of blue nodes. Concretely one of the most commonly used activation layers is the rectified linear unit (ReLU).

![](RhqA-eYXRe-agPnmF4XvZQ_f15b102b5a184826956c21c55b72053f_Screen-Sh.png)

*ReLU(x)* is defined as *max(0,x)* for any input x. 

