# References

This course drew from the following resources:

- <a href="https://arxiv.org/abs/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (Raffel et al, 2019)
- <a href="https://arxiv.org/abs/2001.04451" target="_blank">Reformer: The Efficient Transformer</a> (Kitaev et al, 2020)
- <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a> (Vaswani et al, 2017)
-â€‹<a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank">Deep contextualized word representations</a> (Peters et al, 2018)
- <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a> (Alammar, 2018)
- <a href="http://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> (Alammar, 2019)
- <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al, 2018)
- <a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank">How GPT3 Works - Visualizations and Animations</a> (Alammar, 2020)