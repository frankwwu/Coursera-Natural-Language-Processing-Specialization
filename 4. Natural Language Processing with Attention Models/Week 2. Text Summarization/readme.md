# Learning Objectives

* Describe the three basic types of attention
* Name the two types of layers in a Transformer
* Define three main matrices in attention
* Interpret the math behind scaled dot product attention, causal attention, and multi-head attention
* Use articles and their summaries to create input features for training a text summarizer
* Build a Transformer decoder model (GPT-2)

